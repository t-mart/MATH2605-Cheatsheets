\documentclass[10pt,letterpaper]{article}

\usepackage[landscape,margin=0.1in]{geometry}
\usepackage{multicol}
\usepackage{setspace}
\usepackage[small,compact]{titlesec}
\usepackage{amsmath}
\usepackage{xfrac}

%\setlength{\abovedisplayshortskip}{0pt}
%\setlength{\belowdisplayshortskip}{0pt}
%\setlength{\abovedisplayskip}{0pt}
%\setlength{\belowdisplayskip}{0pt}

\newenvironment{tight_item}
{\begin{itemize}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\topsep}{0pt}
\setlength{\partopsep}{0pt}
\setlength{\leftmargin}{0em}
\setlength{\labelwidth}{0em}
\setlength{\labelsep}{0em} }
{\end{itemize}}

\newenvironment{tight_enum}
{\begin{enumerate}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\topsep}{0pt}
\setlength{\partopsep}{0pt}
\setlength{\leftmargin}{0em}
\setlength{\labelwidth}{0em}
\setlength{\labelsep}{0em} }
{\end{enumerate}}

\newenvironment{tight_desc}
{\begin{description}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\topsep}{0pt}
\setlength{\partopsep}{0pt}
\setlength{\leftmargin}{0em}
\setlength{\labelwidth}{0em}
\setlength{\labelsep}{1em} }
{\end{description}}

\begin{document}
\footnotesize{

\begin{multicols*}{5}

\section{Norms}
\subsection{Vector}
\begin{center}
$\|x\| = \sqrt{x_1^2 + \cdots + x_n^2}$
\end{center}
\subsection{Matrix}
\begin{center}
$\|A\| = \sigma_{1}$
\end{center}

\section{Inverse}
\begin{center}
$AA^{-1} = A^{-1}A = I$
\end{center}
\begin{tight_item}
\item Only invertible if (all equivalent): cols are linearly independent,
$\det{A} \neq 0$, $\operatorname{Null}(A)= \{0\}$, $A \textbf{x} =
\textbf{b}$ has 1 soln, $\operatorname{Rank}(A) =$ \# cols of $A$, 0 is not a
$\lambda$
\item Generally, solved by Gaussian elimination of $\begin{bmatrix}\: A \; | \; I \: \end{bmatrix}$
\item However, $2 \times 2$ can be inverted with
$\frac{1}{\det{A}}\begin{bmatrix}d & -b\\-c & a \end{bmatrix}$
\item $(AB)^{-1} = B^{-1}A^{-1}$ and same for $^{T}$ as well
\end{tight_item}

\section{Spaces}
\begin{tight_desc}
\item[range] The column space of $A = \operatorname{R}(A)$
\item[corange] The column space of $A^{T} = \operatorname{R}(A^{T})$. Also the row space of $A$
\item[kernel] The null space of $A = \operatorname{N}(A)$
\item[cokernel] The null space of $A^{T} = \operatorname{N}(A^{T})$
\end{tight_desc}
\begin{tight_enum}
\item To find row space or col space, find linearly independent vectors of $A$ (i.e. pivot rows/cols) of the original matrix.
\end{tight_enum}

\section{Bases}
\begin{tight_item}
\item A basis is a linearly independent set of vectors
\end{tight_item}
\subsection{Basis of Null Space}
\begin{center}
$A\textbf{x} = \textbf{0}$
\end{center}
\subsection{Basis of Column Space}
\begin{center}
$A\textbf{x} = \textbf{b}$
\end{center}

\section{Change of Basis}
\begin{center}
$C \textbf{a}_{b} = \textbf{a}$
\end{center}
\begin{tight_item}
\item $C$ is "change of basis" matrix, the augmented columns of the new basis b.
\item $\textbf{a}_{b}$ is the vector in the new basis
\item $\textbf{a}$ is the vector in the standard basis
\end{tight_item}
\begin{tight_enum}
\item Plug in what I know, and solve for what I don't
\end{tight_enum}

\section{Linear Transforms}
\begin{center}
$T = B^{-1}AB$
\end{center}
\begin{tight_item}
\item $T$ is a mapping from $\textbf{R}^{n}$ to $\textbf{R}^{n}$, the transformation matrix
\item $B$ is the coefficient matrix of the target basis $V$
\item $A$ is the matrix with respect for the standard basis
\end{tight_item}
\begin{tight_enum}
\item Plug in what I know, and solve for what I don't
\end{tight_enum}

\begin{tight_desc}
\item[Example] \hfill \\
$V = \begin{Bmatrix} \begin{pmatrix}1 \\ 1 \end{pmatrix}, \begin{pmatrix}3 \\ 1
\end{pmatrix} \end{Bmatrix}$ \\
$T \begin{pmatrix} \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} \end{pmatrix} =
\begin{pmatrix} x_{1} + 2x_{2} \\ x_{1} + x_{2} \end{pmatrix}$\\
Determine matrix $T$ in the basis $V$.
\item[Soln] \hfill \\
$T = B^{-1}AB$ \\
$T = \begin{pmatrix} \sfrac{-1}{2} & \sfrac{3}{2} \\ \sfrac{1}{2} & \sfrac{-1}{2}
\end{pmatrix} \begin{pmatrix} 1 & 2 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 1 & 3
\\ 1 & 1 \end{pmatrix}$ \\
$T = \begin{pmatrix} \sfrac{3}{2} & \sfrac{7}{2} \\ \sfrac{1}{2} & \sfrac{1}{2}
\end{pmatrix}$
\end{tight_desc}
\begin{tight_item}
\item The columns of T are the vectors of the new basis
\end{tight_item}

\section{Determinant}
\begin{center}
$
\begin{vmatrix}
a_{1,1} & a_{1,2} \\
a_{2,1} & a_{2,2} \end{vmatrix} = \det{A}
%= (\sum \prod \operatorname{diag}(\searrow)) - (\sum \prod \operatorname{diag}(\swarrow))
$
\end{center}
\begin{tight_item}
\item The sum of the products of the $\searrow$ diagonals minus the sum of
the products of the $\swarrow$ diagonals
\end{tight_item}

\section{Products}
\subsection{Cross Product}
\begin{center}
\begin{math}
\mathbf{a}\times\mathbf{b}=
\begin{vmatrix}
a_2 & a_3\\
b_2 & b_3
\end{vmatrix} \mathbf{i} - 
\begin{vmatrix}
a_1 & a_3\\
b_1 & b_3
\end{vmatrix} \mathbf{j}+
\begin{vmatrix}
a_1 & a_2\\
b_1 & b_2
\end{vmatrix} \mathbf{k}
\end{math}
\end{center}
\subsection{Inner Product}
\begin{center}
$\langle (x_1,\ldots, x_n),(y_1,\ldots, y_n)\rangle := \sum_{i=1}^{n} x_i y_i =
x_1 y_1 + \cdots + x_n y_n$
\end{center}
\begin{tight_item}
\item For matrices, this is called the dot product. $AB$, where $A$ determines row
and $B$ determines column
\end{tight_item}


\section{Eigendecomposition}

\subsection{Eigenvalues}
\begin{center}
$\det{(A - \lambda I)}=0$
\end{center}
\begin{tight_enum}
\item This eqn creates a characteristic polynomial. Solve for the roots.
\end{tight_enum}

\subsection{Eigenvectors}
\begin{center}
$(A- \lambda I)\textbf{x} = 0$
\end{center}
\begin{tight_enum}
\item Find nullspace for a particular $\lambda$
\item Put \textbf{x} in terms of free variables (parametric vector form). This
vector is the eigenvector.
\item If a $\lambda$ is not unique (esp. when finding orthogonal matrix), when
solving for \textbf{x}, there will be 2 or more free vars. Set all but one to 0,
and get eigenvector. Then, $\textbf{x}_{1} \times \textbf{x}_{2} =
\textbf{x}_{3}$
\end{tight_enum}

\subsection{Diagonalization}
\begin{center}
$A = VDV^{-1}$\\
$A = QDQ^{T}$
\end{center}
\begin{tight_item}
\item $A$ is diagonalizable if $\exists$ a nonsingular $V$ or orthogonal $Q$ and diagonal $D$
\item $D$ is the diagonal of $\lambda$s
\end{tight_item}
\begin{tight_enum}
\item Find $\lambda$s
\item Eigenvectors of $A$ are columns of $Q$ in corresponding order to $D$
\end{tight_enum}

\section{Rotation Matrices}

\subsection{Axis of Rotation}
\begin{center}
$(R - I) \textbf{u} = 0$
\end{center}
\begin{tight_enum}
\item Subtract I
\item Solve kernel(R - I)
\item The null space vector is the axis
\end{tight_enum}

\subsection{Angle of Rotation}
\begin{center}
$\operatorname{trace}(R) = 1 + 2\cos(\theta)$
\end{center}
\begin{tight_item}
\item $\operatorname{trace}(R) = \sum_{i=1}^{n} r_{i i}$
\end{tight_item}
\begin{tight_enum}
\item Solve for $\theta$
\end{tight_enum}

\section{SVD}
\begin{center}
$A=UDV^{T}$
\end{center}
\begin{tight_item}
\item $A_{m \times n}$ (where $m > n$, but if not $A^{T}=VDU^{T}$)
\item $D_{k \times k}$ is the singular values along the diagonal (no zeros!)
\item $U_{m\times k}$ and $V_{k \times n}$ are orthogonal
\end{tight_item}
\begin{tight_enum}
\item Find the $\lambda$'s of $AA^{T}$ or $A^{T}A$ depending on which is smaller
in size
\item $D = \operatorname{diag}(\sqrt{\lambda_{i}}) \; \forall \; \lambda_{i}$ in
decreasing order. $\sqrt{\lambda_{i}} = \sigma_{i}$
\item V: Find eigenvectors for each $\lambda$ in $A^{T}A$. These are the columns
of V (keep respective to $\sigma$ order in $D$
\item U: Same as V, but in $AA^{T}$, or just solve for $\textbf{u}_{i} =
\frac{1}{\sigma_{i}} A \textbf{v}_{i}$
\end{tight_enum}
\begin{tight_item}
\item Again, the eigenvalues of $AA^{T}$ or $A^{T}A$ must be $\sqrt{\lambda_{i}}$-ed to
make $D$
\item But, if A is symmetric, no matrix mult needed and $\sigma_{i} =
\lambda_{i}$ of A
\end{tight_item}

\section{QR Decomposition}
\begin{center}
$A = QR$
\end{center}
\begin{tight_item}
\item $A_{m \times n}$
\item $Q_{m \times m}$ is orthogonal (columns are orthonormal)
\item $R_{m \times n}$ is upper triangular
\item $\mathrm{proj}_{\mathbf{u}}\,(\mathbf{v}) = {\langle \mathbf{v},
\mathbf{u}\rangle\over\langle \mathbf{u}, \mathbf{u}\rangle}\mathbf{u}$
\item $\mathrm{comp}_{\mathbf{u}}\,(\mathbf{v}) = {\langle \mathbf{v}, \mathbf{u}\rangle\over \|u\|}$
\end{tight_item}
\subsection{Gram-Schmidt}
$ \mathbf{w}_1 = \mathbf{a}_1 \\ 
   \mathbf{q}_1 = {\mathbf{w}_1 \over \|\mathbf{w}_1\|} \\
 \mathbf{w}_2 = \mathbf{a}_2-\mathrm{proj}_{\mathbf{w}_1}\,\mathbf{a}_2 \\
   \mathbf{q}_2 = {\mathbf{w}_2 \over \|\mathbf{w}_2\|} \\
 \mathbf{w}_3 = \mathbf{a}_3-\mathrm{proj}_{\mathbf{w}_1}\,\mathbf{a}_3-\mathrm{proj}_{\mathbf{w}_2}\,\mathbf{a}_3 \\
   \mathbf{q}_3 = {\mathbf{w}_3 \over \|\mathbf{w}_3\|} \\
$
\begin{center}
$Q = \left[ \mathbf{q}_1, \cdots, \mathbf{q}_n\right] \qquad$
\end{center}
$R = \begin{pmatrix} 
\langle\mathbf{q}_1,\mathbf{a}_1\rangle & \langle\mathbf{q}_1,\mathbf{a}_2\rangle &  \langle\mathbf{q}_1,\mathbf{a}_3\rangle\\
0                & \langle\mathbf{q}_2,\mathbf{a}_2\rangle                        &  \langle\mathbf{q}_2,\mathbf{a}_3\rangle\\
0                & 0                                       & \langle\mathbf{q}_3,\mathbf{a}_3\rangle                        \end{pmatrix}$
\begin{tight_enum}
\item Find the columns of $Q$ by normalizing the $\textbf{w}$ vectors.
\item Find $R$ by rearranging the equations above so that the $\mathbf{a}_i$s are on the left, using the fact that the $\mathbf{q}_i$ are unit vectors:\\
$
 \mathbf{a}_1 = \langle\mathbf{q}_1,\mathbf{a}_1 \rangle \mathbf{q}_1  \\
 \mathbf{a}_2 = \langle\mathbf{q}_1,\mathbf{a}_2 \rangle \mathbf{q}_1 
  + \langle\mathbf{q}_2,\mathbf{a}_2 \rangle \mathbf{q}_2 \\
 \mathbf{a}_3 = \langle\mathbf{q}_1,\mathbf{a}_3 \rangle \mathbf{q}_1 
  + \langle\mathbf{q}_2,\mathbf{a}_3 \rangle \mathbf{q}_2 
  + \langle\mathbf{q}_3,\mathbf{a}_3 \rangle \mathbf{q}_3 \\
 \vdots $ \\
These coefficients are the upper diagonal of $R$ or alternatively, find $R$ s.t. $R =Q^{T}A$
\end{tight_enum}
\subsection{Householder Reflection}
\begin{center}
$\textbf{u} = \textbf{x} + \operatorname{sign}(\textbf{x}_{1}) \textbf{e}_{1} \| \textbf{x} \|$\\
$\textbf{v} = \frac{\textbf{u}}{ \| \textbf{u} \| }$ \\
$Q_{i} = I - 2 \textbf{v} \textbf{v}^{T}$ \\
$R = Q_{n-1},\dots,Q_{1}A$ \\
$Q = Q_{1},\dots,Q_{n-1}$
\end{center}
\begin{tight_item}
\item $e_{1}$ is $\begin{pmatrix}1,0,0,\cdots,0\end{pmatrix}^{T}$
\item $x$ is the first column of $A$ and $x_{1}$ is the first element of $x$
\end{tight_item}
\begin{tight_enum}
\item Find $\textbf{u}$, then $\textbf{v}$, then plug into eqn for $Q_{i}$
\item Multiply $Q_{i}A$
\item If $Q_{i}A$ is upper triangular, $R = Q_{i}A$
\item Else, repeat process with bottom right submatrix of $Q_{i}A$ (delete 1st row and col)
\item And $Q = Q_{1},\dots,Q_{n-1}$
\end{tight_enum}

\subsection{Least Squares}
\begin{tight_item}
\item $A \textbf{x} = \textbf{b}$ may sometimes not have a soln, so we need to
find $\min{ \| A \textbf{x} - \textbf{b} \|}$. The solution of this is the
solution of:
\end{tight_item}
\begin{center}
$R \textbf{x} = Q^{T} \textbf{b}$
\end{center}

\section{LU Decomposition}
\begin{center}
$A = LU$
\end{center}
\begin{tight_item}
\item $L$ is lower triangular
\item $U$ is upper triangular, with 1's on diagonal
\item Don't row swap, or else you need a permutation matrix (?)
\end{tight_item}
\begin{tight_enum}
\item U: Put A in RREF, and keep track of row operations
\item L: The collection of the reciprocals coefficients from those row ops \\
For example, to get a $1$ in $u_{i \times i}$ we had to multiply row $i$ by a factor $x$, with $l_{i \times i} = \sfrac{1}{x}$. Below the diagonal, use the factor that another row is multiplied by and added to s.t. it zeros that position. 
\end{tight_enum}

\section{Conditioning}
\begin{tight_item}
\item Zero can't be a minimum $\lambda$ or $\sigma$
\item High condition numbers indicate sensitivity to matrix operations
\end{tight_item}
\subsection{Symmetric}
\begin{center}
$\operatorname{Cond}A = \frac{| \max{\lambda} |}{| \min{\lambda} |}$
\end{center}
\subsection{General}
\begin{center}
$\operatorname{Cond}A = \frac{\sigma_{1}}{\sigma_{k}}$
\end{center}
\subsection{Partial Pivoting}
\begin{tight_item}
\item Row swapping such that the pivots for row operations are the rows with the
greatest absolute value in the pivot position
\end{tight_item}

\end{multicols*}
}
\end{document}
